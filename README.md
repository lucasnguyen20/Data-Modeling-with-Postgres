# DATA MODELING WITH POSTGRES ETL PIPELINE

### Introduction: 

Nowadays the businesses tend to use data collected from customer activity to take proactive actions such as improving customer experience, discounts and adding more features, in order to, retain their current customer as well as attract new customers. Therefore, a music streaming startup, Spakify, wants to analyze the JSON file dataset from songs, and user activity logs from their streaming app.

### Project objective: 

Based on JSON dataset including songs and user activity logs from Spakify, create a Postgres database with a star schema by defining fact and dimension tables for a particular analytic with optimized queries, and build ETL pipeline to transfer data from files in local directories into tables in Postgres using Python and SQL.

### Dataset

- Song Dataset: a subset of real data, each file in JSON format and contains metadata about a song and the artist of that song.<br>
- Log Dataset: consists of log files in JSON format generated by the event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### Schema for Song Play Analysis

The star schema is used in the project, it consists a main fact table with all variables associated with each event *songplays*, and 4 dimension tables *uses*, *songs*, *artists* and *time* with primary keys referenced from the fact table.

1. Fact table:
    1. **songplays**:  records in log data associated with song plays: *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*.
2. Dimension Tables:
    1. **users** - users in the app : *user_id, first_name, last_name, gender, level*.        
    2. **songs** - songs in music database: *song_id, title, artist_id, year, duration*.      
    3. **artists** - artists in music database: *artist_id, name, location, latitude, longitude*.      
    4. **time** - timestamps of records in **songplays** broken down into specific units: *start_time, hour, day, week, month, year, weekday*.

### Project Instruction:

1. test.ipynb: displays the first few rows of each table to let you check your database.<br>
2. create_tables.py: connect to the database and create a database named Spakify to execute queries in sql_queries.py.<br>
3. etl.ipynb: reads and processes a single file from *song_data* and *log_data* and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.<br>
4. etl.py: reads and processes files from *song_data* and *log_data* and loads them into your tables. You can fill this out based on your work in the ETL notebook.<br>
5. sql_queries.py: contains all SQL queries to create tables, insert values and drop tables, and is imported into the last three files above.


### How to Run:

1. Run *create_tables.py* in terminal to connect with database.<br>
2. Run *etl.py* in terminal to load data, process queries and create tables.<br>
3. Run *test.ipynb* to examine the tables.<br>